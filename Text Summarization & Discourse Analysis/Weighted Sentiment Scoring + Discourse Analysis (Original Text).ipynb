{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04b08a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Label</th>\n",
       "      <th>user_links_dropped</th>\n",
       "      <th>spelling_corrected</th>\n",
       "      <th>hashtag_topics</th>\n",
       "      <th>expanded</th>\n",
       "      <th>wiki_topics</th>\n",
       "      <th>clean_regex</th>\n",
       "      <th>clean_lower</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>remaining_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`- This is not ``creative``.  Those are the di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`- This is not ``creative``. Those are the dic...</td>\n",
       "      <td>`- This is not ``creative``. Those are the dic...</td>\n",
       "      <td>[]</td>\n",
       "      <td>`- This is not ``creative``. Those are the dic...</td>\n",
       "      <td>[]</td>\n",
       "      <td>This is not creative Those are the dictionary...</td>\n",
       "      <td>this is not creative those are the dictionary...</td>\n",
       "      <td>['this', 'is', 'not', 'creative', 'those', 'ar...</td>\n",
       "      <td>['creative', 'dictionary', 'definitions', 'ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>`  :: the term ``standard model`` is itself le...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>` :: the term ``standard model`` is itself les...</td>\n",
       "      <td>` :: the term ``standard model`` is itself les...</td>\n",
       "      <td>[]</td>\n",
       "      <td>` :: the term ``standard model`` is itself les...</td>\n",
       "      <td>[]</td>\n",
       "      <td>the term standard model is itself less NPOV ...</td>\n",
       "      <td>the term standard model is itself less npov ...</td>\n",
       "      <td>['the', 'term', 'standard', 'model', 'is', 'it...</td>\n",
       "      <td>['term', 'standard', 'model', 'less', 'npov', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True or false, the situation as of March 2002 ...</td>\n",
       "      <td>True or false the situation as of March 2002 w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>True or false, the situation as of March 2002 ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>True or false the situation as of March 2002 w...</td>\n",
       "      <td>true or false the situation as of march 2002 w...</td>\n",
       "      <td>['true', 'or', 'false', 'the', 'situation', 'a...</td>\n",
       "      <td>['true', 'false', 'situation', 'march', '2002'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>next maybe you could work on being less condes...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Next maybe you could work on being less condes...</td>\n",
       "      <td>next maybe you could work on being less condes...</td>\n",
       "      <td>['next', 'maybe', 'you', 'could', 'work', 'on'...</td>\n",
       "      <td>['next', 'maybe', 'could', 'work', 'less', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[]</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[]</td>\n",
       "      <td>This page will need disambiguation</td>\n",
       "      <td>this page will need disambiguation</td>\n",
       "      <td>['this', 'page', 'will', 'need', 'disambiguati...</td>\n",
       "      <td>['page', 'need', 'disambiguation']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204954</th>\n",
       "      <td>205173</td>\n",
       "      <td>205173</td>\n",
       "      <td>228((( real!!!! by walt disney=</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>228((( real!!!! by walt disney=</td>\n",
       "      <td>228((( real!!!! by walt disney</td>\n",
       "      <td>[]</td>\n",
       "      <td>228((( real!!!! by walt disney=</td>\n",
       "      <td>[]</td>\n",
       "      <td>228 real by walt disney</td>\n",
       "      <td>228 real by walt disney</td>\n",
       "      <td>['228', 'real', 'by', 'walt', 'disney']</td>\n",
       "      <td>['228', 'real', 'walt', 'disney']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204955</th>\n",
       "      <td>205174</td>\n",
       "      <td>205174</td>\n",
       "      <td>Status-Online Im ZxkillergirlzX! I'm Zxkillerg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Status-Online Im ZxkillergirlzX! I'm Zxkillerg...</td>\n",
       "      <td>Status-Online Im ZxkillergirlzX! I'm Zxkillerg...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Status-Online I Am ZxkillergirlzX! I am Zxkill...</td>\n",
       "      <td>[]</td>\n",
       "      <td>StatusOnline I Am ZxkillergirlzX I am Zxkiller...</td>\n",
       "      <td>statusonline i am zxkillergirlzx i am zxkiller...</td>\n",
       "      <td>['statusonline', 'i', 'am', 'zxkillergirlzx', ...</td>\n",
       "      <td>['statusonline', 'zxkillergirlzx', 'zxkillergi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204956</th>\n",
       "      <td>205175</td>\n",
       "      <td>205175</td>\n",
       "      <td>JR so cute EXO M Better I agree like yeah yeah...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>JR so cute EXO M Better I agree like yeah yeah...</td>\n",
       "      <td>or so cute EXO i Better I agree like yeah yeah...</td>\n",
       "      <td>[]</td>\n",
       "      <td>JR so cute EXO M Better I agree like yeah yeah...</td>\n",
       "      <td>[]</td>\n",
       "      <td>JR so cute EXO M Better I agree like yeah yeah...</td>\n",
       "      <td>jr so cute exo m better i agree like yeah yeah...</td>\n",
       "      <td>['jr', 'so', 'cute', 'exo', 'm', 'better', 'i'...</td>\n",
       "      <td>['jr', 'cute', 'exo', 'better', 'agree', 'like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204957</th>\n",
       "      <td>205176</td>\n",
       "      <td>205176</td>\n",
       "      <td>! !</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>! !</td>\n",
       "      <td>! !</td>\n",
       "      <td>[]</td>\n",
       "      <td>! !</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204958</th>\n",
       "      <td>205177</td>\n",
       "      <td>205177</td>\n",
       "      <td>great video and MERRY CHRISTMAS from greece :*...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>great video and MERRY CHRISTMAS from greece :*...</td>\n",
       "      <td>great video and MERRY CHRISTMAS from greece :*...</td>\n",
       "      <td>[]</td>\n",
       "      <td>great video and MERRY CHRISTMAS from greece :*...</td>\n",
       "      <td>[]</td>\n",
       "      <td>great video and MERRY CHRISTMAS from greece   ...</td>\n",
       "      <td>great video and merry christmas from greece   ...</td>\n",
       "      <td>['great', 'video', 'and', 'merry', 'christmas'...</td>\n",
       "      <td>['great', 'video', 'merry', 'christmas', 'gree...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204959 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Unnamed: 0.1  \\\n",
       "0                0             0   \n",
       "1                1             1   \n",
       "2                2             2   \n",
       "3                3             3   \n",
       "4                4             4   \n",
       "...            ...           ...   \n",
       "204954      205173        205173   \n",
       "204955      205174        205174   \n",
       "204956      205175        205175   \n",
       "204957      205176        205176   \n",
       "204958      205177        205177   \n",
       "\n",
       "                                                     Text  oh_label  Label  \\\n",
       "0       `- This is not ``creative``.  Those are the di...         0      0   \n",
       "1       `  :: the term ``standard model`` is itself le...         0      0   \n",
       "2         True or false, the situation as of March 200...         0      0   \n",
       "3        Next, maybe you could work on being less cond...         0      0   \n",
       "4                    This page will need disambiguation.          0      0   \n",
       "...                                                   ...       ...    ...   \n",
       "204954                    228((( real!!!! by walt disney=         0      0   \n",
       "204955  Status-Online Im ZxkillergirlzX! I'm Zxkillerg...         0      0   \n",
       "204956  JR so cute EXO M Better I agree like yeah yeah...         0      0   \n",
       "204957                                                ! !         0      0   \n",
       "204958  great video and MERRY CHRISTMAS from greece :*...         0      0   \n",
       "\n",
       "                                       user_links_dropped  \\\n",
       "0       `- This is not ``creative``. Those are the dic...   \n",
       "1       ` :: the term ``standard model`` is itself les...   \n",
       "2       True or false, the situation as of March 2002 ...   \n",
       "3       Next, maybe you could work on being less conde...   \n",
       "4                     This page will need disambiguation.   \n",
       "...                                                   ...   \n",
       "204954                    228((( real!!!! by walt disney=   \n",
       "204955  Status-Online Im ZxkillergirlzX! I'm Zxkillerg...   \n",
       "204956  JR so cute EXO M Better I agree like yeah yeah...   \n",
       "204957                                                ! !   \n",
       "204958  great video and MERRY CHRISTMAS from greece :*...   \n",
       "\n",
       "                                       spelling_corrected hashtag_topics  \\\n",
       "0       `- This is not ``creative``. Those are the dic...             []   \n",
       "1       ` :: the term ``standard model`` is itself les...             []   \n",
       "2       True or false the situation as of March 2002 w...             []   \n",
       "3       next maybe you could work on being less condes...             []   \n",
       "4                     This page will need disambiguation.             []   \n",
       "...                                                   ...            ...   \n",
       "204954                     228((( real!!!! by walt disney             []   \n",
       "204955  Status-Online Im ZxkillergirlzX! I'm Zxkillerg...             []   \n",
       "204956  or so cute EXO i Better I agree like yeah yeah...             []   \n",
       "204957                                                ! !             []   \n",
       "204958  great video and MERRY CHRISTMAS from greece :*...             []   \n",
       "\n",
       "                                                 expanded wiki_topics  \\\n",
       "0       `- This is not ``creative``. Those are the dic...          []   \n",
       "1       ` :: the term ``standard model`` is itself les...          []   \n",
       "2       True or false, the situation as of March 2002 ...          []   \n",
       "3       Next, maybe you could work on being less conde...          []   \n",
       "4                     This page will need disambiguation.          []   \n",
       "...                                                   ...         ...   \n",
       "204954                    228((( real!!!! by walt disney=          []   \n",
       "204955  Status-Online I Am ZxkillergirlzX! I am Zxkill...          []   \n",
       "204956  JR so cute EXO M Better I agree like yeah yeah...          []   \n",
       "204957                                                ! !          []   \n",
       "204958  great video and MERRY CHRISTMAS from greece :*...          []   \n",
       "\n",
       "                                              clean_regex  \\\n",
       "0        This is not creative Those are the dictionary...   \n",
       "1         the term standard model is itself less NPOV ...   \n",
       "2       True or false the situation as of March 2002 w...   \n",
       "3       Next maybe you could work on being less condes...   \n",
       "4                      This page will need disambiguation   \n",
       "...                                                   ...   \n",
       "204954                            228 real by walt disney   \n",
       "204955  StatusOnline I Am ZxkillergirlzX I am Zxkiller...   \n",
       "204956  JR so cute EXO M Better I agree like yeah yeah...   \n",
       "204957                                                      \n",
       "204958  great video and MERRY CHRISTMAS from greece   ...   \n",
       "\n",
       "                                              clean_lower  \\\n",
       "0        this is not creative those are the dictionary...   \n",
       "1         the term standard model is itself less npov ...   \n",
       "2       true or false the situation as of march 2002 w...   \n",
       "3       next maybe you could work on being less condes...   \n",
       "4                      this page will need disambiguation   \n",
       "...                                                   ...   \n",
       "204954                            228 real by walt disney   \n",
       "204955  statusonline i am zxkillergirlzx i am zxkiller...   \n",
       "204956  jr so cute exo m better i agree like yeah yeah...   \n",
       "204957                                                      \n",
       "204958  great video and merry christmas from greece   ...   \n",
       "\n",
       "                                           tokenized_text  \\\n",
       "0       ['this', 'is', 'not', 'creative', 'those', 'ar...   \n",
       "1       ['the', 'term', 'standard', 'model', 'is', 'it...   \n",
       "2       ['true', 'or', 'false', 'the', 'situation', 'a...   \n",
       "3       ['next', 'maybe', 'you', 'could', 'work', 'on'...   \n",
       "4       ['this', 'page', 'will', 'need', 'disambiguati...   \n",
       "...                                                   ...   \n",
       "204954            ['228', 'real', 'by', 'walt', 'disney']   \n",
       "204955  ['statusonline', 'i', 'am', 'zxkillergirlzx', ...   \n",
       "204956  ['jr', 'so', 'cute', 'exo', 'm', 'better', 'i'...   \n",
       "204957                                                 []   \n",
       "204958  ['great', 'video', 'and', 'merry', 'christmas'...   \n",
       "\n",
       "                                          remaining_words  \n",
       "0       ['creative', 'dictionary', 'definitions', 'ter...  \n",
       "1       ['term', 'standard', 'model', 'less', 'npov', ...  \n",
       "2       ['true', 'false', 'situation', 'march', '2002'...  \n",
       "3       ['next', 'maybe', 'could', 'work', 'less', 'co...  \n",
       "4                      ['page', 'need', 'disambiguation']  \n",
       "...                                                   ...  \n",
       "204954                  ['228', 'real', 'walt', 'disney']  \n",
       "204955  ['statusonline', 'zxkillergirlzx', 'zxkillergi...  \n",
       "204956  ['jr', 'cute', 'exo', 'better', 'agree', 'like...  \n",
       "204957                                                 []  \n",
       "204958  ['great', 'video', 'merry', 'christmas', 'gree...  \n",
       "\n",
       "[204959 rows x 14 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"clean2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a29818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    178410\n",
       "1     25879\n",
       "2       670\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0d70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling\n",
    "df = df.groupby('Label', group_keys=False).apply(lambda x: x.sample(670))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cdbeb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    670\n",
       "1    670\n",
       "2    670\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241af952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5bfc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever|Hey|Hi|I|You)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [re.sub(r'[^\\w\\s]|_', '', s) for s in sentences]\n",
    "    sentences = [s.lower() for s in sentences]\n",
    "    sentences = [s.strip() for s in sentences] \n",
    "    sentences = list(filter(None, sentences))\n",
    "    return sentences\n",
    "\n",
    "df['sentences'] = df['expanded'].apply(split_into_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "050c34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(sentence_list):\n",
    "    paragraph = \". \".join(sentence_list)\n",
    "    sentiment_score = analyzer.polarity_scores(paragraph)['compound']\n",
    "    return sentiment_score\n",
    "\n",
    "df['Sentiment Score (Original Text)'] = df['sentences'].apply(get_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e353b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment Score (PDTB split)'] = 0\n",
    "df['Weighted Discourse Fragments (PDTB)'] = ''\n",
    "df['Sentiment Score (Dependency split)'] = 0\n",
    "df['Weighted Discourse Fragments (Dependency split)'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31b84e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['after everything',\n",
       " 'all the same',\n",
       " 'alternatively',\n",
       " 'although',\n",
       " 'anyroad',\n",
       " 'anyway',\n",
       " 'anyhow',\n",
       " 'anywho',\n",
       " 'anywise',\n",
       " 'at any cost',\n",
       " 'au contraire',\n",
       " 'be that as it may',\n",
       " 'be it as it may',\n",
       " 'besides',\n",
       " 'but',\n",
       " 'but for all that',\n",
       " 'by contrast',\n",
       " 'come what may',\n",
       " 'contradictorily',\n",
       " 'contrastingly',\n",
       " 'contrarily',\n",
       " 'contrariwise',\n",
       " 'despite',\n",
       " 'despite that',\n",
       " 'despite this fact',\n",
       " 'even so',\n",
       " 'either way',\n",
       " 'conversely',\n",
       " 'for all that',\n",
       " 'furthermore',\n",
       " 'having said that',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'in any case',\n",
       " 'in any event',\n",
       " 'in contrast',\n",
       " 'instead',\n",
       " 'in spite of',\n",
       " 'in opposition',\n",
       " 'inversely',\n",
       " 'irregardless',\n",
       " 'just the same',\n",
       " 'meanwhile',\n",
       " 'nevertheless',\n",
       " 'natheless',\n",
       " 'no matter',\n",
       " 'no matter what',\n",
       " 'nonetheless',\n",
       " 'notwithstanding',\n",
       " 'on one hand',\n",
       " 'on the flip side',\n",
       " 'on the other hand',\n",
       " 'on the other side of the coin',\n",
       " 'on the contrary',\n",
       " 'oppositely',\n",
       " 'rather',\n",
       " 'regardless',\n",
       " 'still',\n",
       " 'still and all',\n",
       " 'then again',\n",
       " 'that said',\n",
       " 'though',\n",
       " 'to the contracy',\n",
       " 'under any circumstances',\n",
       " 'whatevs',\n",
       " 'whatever',\n",
       " 'whatever happens',\n",
       " 'whatever the case',\n",
       " 'whatever the cost',\n",
       " 'with that being said',\n",
       " 'whereas',\n",
       " 'withal',\n",
       " 'yet']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('contrastive_markers.txt') as f:\n",
    "    contrastive_markers = f.readlines()\n",
    "contrastive_markers = [re.sub(r'\\n', '', marker) for marker in contrastive_markers]\n",
    "contrastive_markers.pop(0)\n",
    "contrastive_markers = [re.sub(r'[^\\w\\s]|_', '', marker) for marker in contrastive_markers]\n",
    "contrastive_markers = [marker.strip() for marker in contrastive_markers]\n",
    "contrastive_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d11fe8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discourse_segmentation(sentence_list):\n",
    "    weighted_sentences = {}\n",
    "    for segment in sentence_list:\n",
    "        segment_list = segment.split(\" \")\n",
    "        discourse_found = set(segment_list).intersection(contrastive_markers)\n",
    "        \n",
    "        weight = 1\n",
    "        if len(discourse_found) == 0:\n",
    "            weighted_sentences[segment] = weight\n",
    "        else:\n",
    "            while len(discourse_found) > 0:\n",
    "                discourse = discourse_found.pop()\n",
    "                seg = segment.split(discourse)\n",
    "                \n",
    "                seg = list(filter(None, seg))    \n",
    "                seg = [s.strip() for s in seg] \n",
    "                \n",
    "                for i in range(len(seg)):\n",
    "                    if len(seg) > 1:\n",
    "                        weighted_sentences[seg[i]] = weight\n",
    "                        weight = weight * 1.5 \n",
    "                    else:\n",
    "                        weighted_sentences[seg[i]] = weight\n",
    "    return weighted_sentences\n",
    "\n",
    "df['Weighted Discourse Fragments (PDTB)'] = df['sentences'].apply(discourse_segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "708b890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suck',\n",
       " 'sucks',\n",
       " 'sck',\n",
       " 'suuk',\n",
       " 'fk',\n",
       " 'fuck',\n",
       " 'fck',\n",
       " 'bitch',\n",
       " 'biatch',\n",
       " 'shit',\n",
       " 'shitty',\n",
       " 'sht',\n",
       " 'shtty',\n",
       " 'cocky',\n",
       " 'damn',\n",
       " 'wtf',\n",
       " 'jesus']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('profanities.txt') as f:\n",
    "    profanities = f.readlines()\n",
    "profanities = [re.sub(r'\\n', '', profanity) for profanity in profanities]\n",
    "profanities = [re.sub(r'[^\\w\\s]|_', '', profanity) for profanity in profanities]\n",
    "profanities = [profanity.strip() for profanity in profanities]\n",
    "profanities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "736a157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_sentiment_score(weighted_dictionary):\n",
    "    total_score = 0\n",
    "    \n",
    "    for key, values in weighted_dictionary.copy().items():\n",
    "        # find if any profanities and remove them, as well as add weights to profanity \n",
    "        word_list = key.split(\" \")\n",
    "        \n",
    "        if len(set(word_list).intersection(profanities)) > 0:\n",
    "            # locate the original key of dictionary then update the key and the value to without profanity\n",
    "            original = \" \".join(word_list)\n",
    "            weighted_dictionary[original] =  weighted_dictionary[original] * 1.5\n",
    "            word_list.remove(set(word_list).intersection(profanities).pop())\n",
    "            new_key = \" \".join(word_list)\n",
    "            weighted_dictionary[new_key] = weighted_dictionary.pop(key)\n",
    "    \n",
    "    # normalization of all weights such that sum of weights for all sentences will always add up to 1 \n",
    "    sum_weights = sum(weighted_dictionary.values())\n",
    "    scaled_weights = {key: ((v) / (sum_weights) )  for (key, v) in weighted_dictionary.items() }\n",
    "\n",
    "    # sentiment scoring \n",
    "    for key, values in scaled_weights.items():\n",
    "        total_score += analyzer.polarity_scores(key)['compound'] * scaled_weights[key]\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "df['Sentiment Score (PDTB split)'] = df['Weighted Discourse Fragments (PDTB)'].apply(get_final_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "540967bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f15af4f16945688380f32c07d4db0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 00:50:57 INFO: Downloading default packages for language: en (English)...\n",
      "2022-03-31 00:51:00 INFO: File exists: C:\\Users\\lingl\\stanza_resources\\en\\default.zip\n",
      "2022-03-31 00:51:12 INFO: Finished downloading models and saved to C:\\Users\\lingl\\stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1c2c7a5c5041d69bf77a95209d9564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 00:51:16 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-03-31 00:51:16 INFO: Use device: cpu\n",
      "2022-03-31 00:51:16 INFO: Loading: tokenize\n",
      "2022-03-31 00:51:16 INFO: Loading: pos\n",
      "2022-03-31 00:51:16 INFO: Loading: lemma\n",
      "2022-03-31 00:51:16 INFO: Loading: depparse\n",
      "2022-03-31 00:51:17 INFO: Loading: sentiment\n",
      "2022-03-31 00:51:17 INFO: Loading: constituency\n",
      "2022-03-31 00:51:20 INFO: Loading: ner\n",
      "2022-03-31 00:51:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "49061823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word_id, sentence_list, document):\n",
    "    for i in range(len(sentence_list)):\n",
    "        for records in document.sentences[i].dependencies:\n",
    "            no_str = [word for word in records if not isinstance(word,str)]\n",
    "            for record in no_str:\n",
    "                if record.id == word_id:\n",
    "                    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8a974588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discourse_segments(sentences_list):\n",
    "    entire_sentence = \".\".join(sentences_list)\n",
    "    doc = nlp(entire_sentence)\n",
    "    discourse_marker_sentences = []\n",
    "    for sentence in doc.sentences:\n",
    "        for records in sentence.dependencies:\n",
    "            no_str = [word for word in records if not isinstance(word,str)]\n",
    "            for record in no_str:\n",
    "                if record.deprel == \"mark\":\n",
    "                    discourse_marker_sentence = sentences_list[get_index(record.id, sentences_list, doc)]\n",
    "                    discourse_parent_index = get_index(record.head, sentences_list, doc)\n",
    "                    discourse_parent_sentence = sentences_list[discourse_parent_index]\n",
    "                    if discourse_marker_sentence != discourse_parent_sentence:\n",
    "                        joined = discourse_marker_sentence + \". \" + discourse_parent_sentence\n",
    "                        if joined not in discourse_marker_sentences:\n",
    "                            discourse_marker_sentences.append(joined)\n",
    "                    elif discourse_marker_sentence == discourse_parent_sentence:\n",
    "                        if discourse_marker_sentence not in discourse_marker_sentences:\n",
    "                            discourse_marker_sentences.append(discourse_marker_sentence)    \n",
    "                elif sentences_list[get_index(record.id, sentences_list, doc)] not in discourse_marker_sentences:\n",
    "                    discourse_marker_sentences.append(sentences_list[get_index(record.id, sentences_list, doc)])\n",
    "    return discourse_marker_sentences\n",
    "df['Weighted Discourse Fragments (Dependency split)'] = df['sentences'].apply(get_discourse_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ccffa59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_dict(sentences_list):\n",
    "    sentence_dict = {}\n",
    "    for i in sentences_list:\n",
    "        sentence_dict[i] = 1\n",
    "    return sentence_dict\n",
    "\n",
    "df['Weighted Discourse Fragments (Dependency split)'] = df['Weighted Discourse Fragments (Dependency split)'].apply(change_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c8757243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment Score (Dependency split)'] = df['Weighted Discourse Fragments (Dependency split)'].apply(get_final_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "deeacd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"discourse_segments_sentiment_scored.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
