{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wF3EW1ylxmlF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TU1JucqUxwr6",
    "outputId": "57f2935e-9a99-4727-a1e1-5049acd0a6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xaUVBRiaxyc5",
    "outputId": "f4924db5-319a-4ffd-f644-0221b2b38ef4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d25f5d8a-561f-4c2a-88fa-fd616cf3fcd3\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Label</th>\n",
       "      <th>user_links_dropped</th>\n",
       "      <th>spelling_corrected</th>\n",
       "      <th>hashtag_topics</th>\n",
       "      <th>...</th>\n",
       "      <th>clean_lower</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>remaining_words</th>\n",
       "      <th>sentences</th>\n",
       "      <th>Sentiment Score (Original Text)</th>\n",
       "      <th>summarised_sentences</th>\n",
       "      <th>Weighted Discourse Fragments (PDTB)</th>\n",
       "      <th>Sentiment Score (PDTB split)</th>\n",
       "      <th>Weighted Discourse Fragments (Dependency split)</th>\n",
       "      <th>Sentiment Score (Dependency split)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>149970</td>\n",
       "      <td>149988</td>\n",
       "      <td>149988</td>\n",
       "      <td>`  == Clandestine industries ==  Hi - I note y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>` == Clandestine industries == Hi - I note you...</td>\n",
       "      <td>` == Clandestine industries == Hi - I note you...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>clandestine industries  hi  i note you have ...</td>\n",
       "      <td>['clandestine', 'industries', 'hi', 'i', 'note...</td>\n",
       "      <td>['clandestine', 'industries', 'hi', 'note', 'r...</td>\n",
       "      <td>['clandestine industries  hi  i note you have ...</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>['i note you have removed the speedy deletion ...</td>\n",
       "      <td>{'i note you have removed the speedy deletion ...</td>\n",
       "      <td>0.135575</td>\n",
       "      <td>{'i note you have removed the speedy deletion ...</td>\n",
       "      <td>0.27115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>136680</td>\n",
       "      <td>136697</td>\n",
       "      <td>136697</td>\n",
       "      <td>`  ==Sailor Moon Musicals== I tried to add the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>` ==Sailor Moon Musicals== I tried to add the ...</td>\n",
       "      <td>` ==Sailor Moon Musicals== I tried to add the ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>sailor moon musicals i tried to add the clari...</td>\n",
       "      <td>['sailor', 'moon', 'musicals', 'i', 'tried', '...</td>\n",
       "      <td>['sailor', 'moon', 'musicals', 'tried', 'add',...</td>\n",
       "      <td>['sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>['sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>{'sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>0.122480</td>\n",
       "      <td>{'sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>90772</td>\n",
       "      <td>90780</td>\n",
       "      <td>90780</td>\n",
       "      <td>:::That was my point. I wanted to rewrite the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>:::That was my point. I wanted to rewrite the ...</td>\n",
       "      <td>:::That was my point I wanted to rewrite the e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>that was my point i wanted to rewrite the enti...</td>\n",
       "      <td>['that', 'was', 'my', 'point', 'i', 'wanted', ...</td>\n",
       "      <td>['point', 'wanted', 'rewrite', 'entire', 'plot...</td>\n",
       "      <td>['that was my point', 'i wanted to rewrite the...</td>\n",
       "      <td>0.9674</td>\n",
       "      <td>['that was my point', 'i wanted to rewrite the...</td>\n",
       "      <td>{'that was my point': 1, 'i wanted to rewrite ...</td>\n",
       "      <td>0.168560</td>\n",
       "      <td>{'that was my point': 1, 'i wanted to rewrite ...</td>\n",
       "      <td>0.29360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>177127</td>\n",
       "      <td>177147</td>\n",
       "      <td>177147</td>\n",
       "      <td>`  == Channel 4 Documentary ==  Nice work Prio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>` == Channel 4 Documentary == Nice work Priory...</td>\n",
       "      <td>` == Channel 4 Documentary == Nice work Priory...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>channel 4 documentary  nice work prioryman i...</td>\n",
       "      <td>['channel', '4', 'documentary', 'nice', 'work'...</td>\n",
       "      <td>['channel', '4', 'documentary', 'nice', 'work'...</td>\n",
       "      <td>['channel 4 documentary  nice work prioryman',...</td>\n",
       "      <td>0.7641</td>\n",
       "      <td>['channel 4 documentary   nice work prioryman'...</td>\n",
       "      <td>{'channel 4 documentary   nice work prioryman'...</td>\n",
       "      <td>0.271350</td>\n",
       "      <td>{'channel 4 documentary   nice work prioryman'...</td>\n",
       "      <td>0.32250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>122009</td>\n",
       "      <td>122025</td>\n",
       "      <td>122025</td>\n",
       "      <td>Is this species named after Sir David Attenb...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is this species named after Sir David Attenbor...</td>\n",
       "      <td>Is this species named after Sir David Attenbor...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>is this species named after sir david attenbor...</td>\n",
       "      <td>['is', 'this', 'species', 'named', 'after', 's...</td>\n",
       "      <td>['species', 'named', 'sir', 'david', 'attenbor...</td>\n",
       "      <td>['is this species named after sir david attenb...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>['is this species named after sir david attenb...</td>\n",
       "      <td>{'is this species named after sir david attenb...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'is this species named after sir david attenb...</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>2005</td>\n",
       "      <td>6218</td>\n",
       "      <td>6218</td>\n",
       "      <td>6218</td>\n",
       "      <td>==series scrapped on 4th july??!?==  on http...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>==series scrapped on 4th july??!?== on a few p...</td>\n",
       "      <td>==series scrapped on ith july??!?== on a few p...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>series scrapped on 4th july on a few ppl hav l...</td>\n",
       "      <td>['series', 'scrapped', 'on', '4th', 'july', 'o...</td>\n",
       "      <td>['series', 'scrapped', '4th', 'july', 'ppl', '...</td>\n",
       "      <td>['series scrapped on 4th july', 'on a few ppl ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>['series scrapped on 4th july', 'on a few ppl ...</td>\n",
       "      <td>{'series scrapped on 4th july': 1, 'on a few p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'series scrapped on 4th july': 1, 'on a few p...</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>2006</td>\n",
       "      <td>24195</td>\n",
       "      <td>24196</td>\n",
       "      <td>24196</td>\n",
       "      <td>@Iloveoldtools @Angry_Feminazi No.  Actually, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>No. Actually, she has an economics degree.</td>\n",
       "      <td>not actually she has an economics degrees</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>no actually she has an economics degree</td>\n",
       "      <td>['no', 'actually', 'she', 'has', 'an', 'econom...</td>\n",
       "      <td>['actually', 'economics', 'degree']</td>\n",
       "      <td>['no', 'actually she has an economics degree']</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>['actually she has an economics degree']</td>\n",
       "      <td>{'actually she has an economics degree': 1}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'actually she has an economics degree': 1}</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2007</td>\n",
       "      <td>10847</td>\n",
       "      <td>10847</td>\n",
       "      <td>10847</td>\n",
       "      <td>Liar, Liar, pants on fire Seriously I looked a...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Liar, Liar, pants on fire Seriously I looked a...</td>\n",
       "      <td>liar liar pants on fire Seriously I looked at ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>liar liar pants on fire seriously i looked at ...</td>\n",
       "      <td>['liar', 'liar', 'pants', 'on', 'fire', 'serio...</td>\n",
       "      <td>['liar', 'liar', 'pants', 'fire', 'seriously',...</td>\n",
       "      <td>['liar liar pants on fire seriously i looked a...</td>\n",
       "      <td>-0.8527</td>\n",
       "      <td>['seriously i looked at your contributions the...</td>\n",
       "      <td>{'seriously i looked at your contributions the...</td>\n",
       "      <td>-0.061100</td>\n",
       "      <td>{'seriously i looked at your contributions the...</td>\n",
       "      <td>-0.44040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2008</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>I Love to eat rectal yoghurt</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>I Love to eat rectal yoghurt</td>\n",
       "      <td>I Love to eat rectal yoghurt</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>i love to eat rectal yoghurt</td>\n",
       "      <td>['i', 'love', 'to', 'eat', 'rectal', 'yoghurt']</td>\n",
       "      <td>['love', 'eat', 'rectal', 'yoghurt']</td>\n",
       "      <td>['i love to eat rectal yoghurt']</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>['i love to eat rectal yoghurt']</td>\n",
       "      <td>{'i love to eat rectal yoghurt': 1}</td>\n",
       "      <td>0.636900</td>\n",
       "      <td>{'i love to eat rectal yoghurt': 1}</td>\n",
       "      <td>0.63690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>2009</td>\n",
       "      <td>6636</td>\n",
       "      <td>6636</td>\n",
       "      <td>6636</td>\n",
       "      <td>== Dude I'm trying to make a joke for my fri...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>== Dude I'm trying to make a joke for my frien...</td>\n",
       "      <td>== Dude I'm trying to make a joke for my frien...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>dude i am trying to make a joke for my friend...</td>\n",
       "      <td>['dude', 'i', 'am', 'trying', 'to', 'make', 'a...</td>\n",
       "      <td>['dude', 'trying', 'make', 'joke', 'friend', '...</td>\n",
       "      <td>['dude i am trying to make a joke for my frien...</td>\n",
       "      <td>0.4075</td>\n",
       "      <td>['dude i am trying to make a joke for my frien...</td>\n",
       "      <td>{'dude i am trying to make a joke for my frien...</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>{'dude i am trying to make a joke for my frien...</td>\n",
       "      <td>0.26935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2010 rows Ã— 23 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d25f5d8a-561f-4c2a-88fa-fd616cf3fcd3')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d25f5d8a-561f-4c2a-88fa-fd616cf3fcd3 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d25f5d8a-561f-4c2a-88fa-fd616cf3fcd3');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n",
       "0              0        149970          149988            149988   \n",
       "1              1        136680          136697            136697   \n",
       "2              2         90772           90780             90780   \n",
       "3              3        177127          177147            177147   \n",
       "4              4        122009          122025            122025   \n",
       "...          ...           ...             ...               ...   \n",
       "2005        2005          6218            6218              6218   \n",
       "2006        2006         24195           24196             24196   \n",
       "2007        2007         10847           10847             10847   \n",
       "2008        2008           128             128               128   \n",
       "2009        2009          6636            6636              6636   \n",
       "\n",
       "                                                   Text  oh_label  Label  \\\n",
       "0     `  == Clandestine industries ==  Hi - I note y...         0      0   \n",
       "1     `  ==Sailor Moon Musicals== I tried to add the...         0      0   \n",
       "2      :::That was my point. I wanted to rewrite the...         0      0   \n",
       "3     `  == Channel 4 Documentary ==  Nice work Prio...         0      0   \n",
       "4       Is this species named after Sir David Attenb...         0      0   \n",
       "...                                                 ...       ...    ...   \n",
       "2005    ==series scrapped on 4th july??!?==  on http...         1      2   \n",
       "2006  @Iloveoldtools @Angry_Feminazi No.  Actually, ...         1      2   \n",
       "2007  Liar, Liar, pants on fire Seriously I looked a...         1      2   \n",
       "2008                       I Love to eat rectal yoghurt         1      2   \n",
       "2009    == Dude I'm trying to make a joke for my fri...         1      2   \n",
       "\n",
       "                                     user_links_dropped  \\\n",
       "0     ` == Clandestine industries == Hi - I note you...   \n",
       "1     ` ==Sailor Moon Musicals== I tried to add the ...   \n",
       "2     :::That was my point. I wanted to rewrite the ...   \n",
       "3     ` == Channel 4 Documentary == Nice work Priory...   \n",
       "4     Is this species named after Sir David Attenbor...   \n",
       "...                                                 ...   \n",
       "2005  ==series scrapped on 4th july??!?== on a few p...   \n",
       "2006         No. Actually, she has an economics degree.   \n",
       "2007  Liar, Liar, pants on fire Seriously I looked a...   \n",
       "2008                       I Love to eat rectal yoghurt   \n",
       "2009  == Dude I'm trying to make a joke for my frien...   \n",
       "\n",
       "                                     spelling_corrected hashtag_topics  ...  \\\n",
       "0     ` == Clandestine industries == Hi - I note you...             []  ...   \n",
       "1     ` ==Sailor Moon Musicals== I tried to add the ...             []  ...   \n",
       "2     :::That was my point I wanted to rewrite the e...             []  ...   \n",
       "3     ` == Channel 4 Documentary == Nice work Priory...             []  ...   \n",
       "4     Is this species named after Sir David Attenbor...             []  ...   \n",
       "...                                                 ...            ...  ...   \n",
       "2005  ==series scrapped on ith july??!?== on a few p...             []  ...   \n",
       "2006          not actually she has an economics degrees             []  ...   \n",
       "2007  liar liar pants on fire Seriously I looked at ...             []  ...   \n",
       "2008                       I Love to eat rectal yoghurt             []  ...   \n",
       "2009  == Dude I'm trying to make a joke for my frien...             []  ...   \n",
       "\n",
       "                                            clean_lower  \\\n",
       "0       clandestine industries  hi  i note you have ...   \n",
       "1      sailor moon musicals i tried to add the clari...   \n",
       "2     that was my point i wanted to rewrite the enti...   \n",
       "3       channel 4 documentary  nice work prioryman i...   \n",
       "4     is this species named after sir david attenbor...   \n",
       "...                                                 ...   \n",
       "2005  series scrapped on 4th july on a few ppl hav l...   \n",
       "2006            no actually she has an economics degree   \n",
       "2007  liar liar pants on fire seriously i looked at ...   \n",
       "2008                       i love to eat rectal yoghurt   \n",
       "2009   dude i am trying to make a joke for my friend...   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "0     ['clandestine', 'industries', 'hi', 'i', 'note...   \n",
       "1     ['sailor', 'moon', 'musicals', 'i', 'tried', '...   \n",
       "2     ['that', 'was', 'my', 'point', 'i', 'wanted', ...   \n",
       "3     ['channel', '4', 'documentary', 'nice', 'work'...   \n",
       "4     ['is', 'this', 'species', 'named', 'after', 's...   \n",
       "...                                                 ...   \n",
       "2005  ['series', 'scrapped', 'on', '4th', 'july', 'o...   \n",
       "2006  ['no', 'actually', 'she', 'has', 'an', 'econom...   \n",
       "2007  ['liar', 'liar', 'pants', 'on', 'fire', 'serio...   \n",
       "2008    ['i', 'love', 'to', 'eat', 'rectal', 'yoghurt']   \n",
       "2009  ['dude', 'i', 'am', 'trying', 'to', 'make', 'a...   \n",
       "\n",
       "                                        remaining_words  \\\n",
       "0     ['clandestine', 'industries', 'hi', 'note', 'r...   \n",
       "1     ['sailor', 'moon', 'musicals', 'tried', 'add',...   \n",
       "2     ['point', 'wanted', 'rewrite', 'entire', 'plot...   \n",
       "3     ['channel', '4', 'documentary', 'nice', 'work'...   \n",
       "4     ['species', 'named', 'sir', 'david', 'attenbor...   \n",
       "...                                                 ...   \n",
       "2005  ['series', 'scrapped', '4th', 'july', 'ppl', '...   \n",
       "2006                ['actually', 'economics', 'degree']   \n",
       "2007  ['liar', 'liar', 'pants', 'fire', 'seriously',...   \n",
       "2008               ['love', 'eat', 'rectal', 'yoghurt']   \n",
       "2009  ['dude', 'trying', 'make', 'joke', 'friend', '...   \n",
       "\n",
       "                                              sentences  \\\n",
       "0     ['clandestine industries  hi  i note you have ...   \n",
       "1     ['sailor moon musicals i tried to add the clar...   \n",
       "2     ['that was my point', 'i wanted to rewrite the...   \n",
       "3     ['channel 4 documentary  nice work prioryman',...   \n",
       "4     ['is this species named after sir david attenb...   \n",
       "...                                                 ...   \n",
       "2005  ['series scrapped on 4th july', 'on a few ppl ...   \n",
       "2006     ['no', 'actually she has an economics degree']   \n",
       "2007  ['liar liar pants on fire seriously i looked a...   \n",
       "2008                   ['i love to eat rectal yoghurt']   \n",
       "2009  ['dude i am trying to make a joke for my frien...   \n",
       "\n",
       "     Sentiment Score (Original Text)  \\\n",
       "0                             0.3182   \n",
       "1                             0.3612   \n",
       "2                             0.9674   \n",
       "3                             0.7641   \n",
       "4                             0.0000   \n",
       "...                              ...   \n",
       "2005                          0.0000   \n",
       "2006                          0.0000   \n",
       "2007                         -0.8527   \n",
       "2008                          0.6369   \n",
       "2009                          0.4075   \n",
       "\n",
       "                                   summarised_sentences  \\\n",
       "0     ['i note you have removed the speedy deletion ...   \n",
       "1     ['sailor moon musicals i tried to add the clar...   \n",
       "2     ['that was my point', 'i wanted to rewrite the...   \n",
       "3     ['channel 4 documentary   nice work prioryman'...   \n",
       "4     ['is this species named after sir david attenb...   \n",
       "...                                                 ...   \n",
       "2005  ['series scrapped on 4th july', 'on a few ppl ...   \n",
       "2006           ['actually she has an economics degree']   \n",
       "2007  ['seriously i looked at your contributions the...   \n",
       "2008                   ['i love to eat rectal yoghurt']   \n",
       "2009  ['dude i am trying to make a joke for my frien...   \n",
       "\n",
       "                    Weighted Discourse Fragments (PDTB)  \\\n",
       "0     {'i note you have removed the speedy deletion ...   \n",
       "1     {'sailor moon musicals i tried to add the clar...   \n",
       "2     {'that was my point': 1, 'i wanted to rewrite ...   \n",
       "3     {'channel 4 documentary   nice work prioryman'...   \n",
       "4     {'is this species named after sir david attenb...   \n",
       "...                                                 ...   \n",
       "2005  {'series scrapped on 4th july': 1, 'on a few p...   \n",
       "2006        {'actually she has an economics degree': 1}   \n",
       "2007  {'seriously i looked at your contributions the...   \n",
       "2008                {'i love to eat rectal yoghurt': 1}   \n",
       "2009  {'dude i am trying to make a joke for my frien...   \n",
       "\n",
       "      Sentiment Score (PDTB split)  \\\n",
       "0                         0.135575   \n",
       "1                         0.122480   \n",
       "2                         0.168560   \n",
       "3                         0.271350   \n",
       "4                         0.000000   \n",
       "...                            ...   \n",
       "2005                      0.000000   \n",
       "2006                      0.000000   \n",
       "2007                     -0.061100   \n",
       "2008                      0.636900   \n",
       "2009                      0.080900   \n",
       "\n",
       "        Weighted Discourse Fragments (Dependency split)  \\\n",
       "0     {'i note you have removed the speedy deletion ...   \n",
       "1     {'sailor moon musicals i tried to add the clar...   \n",
       "2     {'that was my point': 1, 'i wanted to rewrite ...   \n",
       "3     {'channel 4 documentary   nice work prioryman'...   \n",
       "4     {'is this species named after sir david attenb...   \n",
       "...                                                 ...   \n",
       "2005  {'series scrapped on 4th july': 1, 'on a few p...   \n",
       "2006        {'actually she has an economics degree': 1}   \n",
       "2007  {'seriously i looked at your contributions the...   \n",
       "2008                {'i love to eat rectal yoghurt': 1}   \n",
       "2009  {'dude i am trying to make a joke for my frien...   \n",
       "\n",
       "     Sentiment Score (Dependency split)  \n",
       "0                               0.27115  \n",
       "1                               0.00000  \n",
       "2                               0.29360  \n",
       "3                               0.32250  \n",
       "4                               0.00000  \n",
       "...                                 ...  \n",
       "2005                            0.00000  \n",
       "2006                            0.00000  \n",
       "2007                           -0.44040  \n",
       "2008                            0.63690  \n",
       "2009                            0.26935  \n",
       "\n",
       "[2010 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "raw = pd.read_csv(\"/content/drive/MyDrive/TM_Dataset/summarised_weighted_discourse.csv\")\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wuDNWkGzR_2",
    "outputId": "3e744441-383c-4a2b-f411-359a38a107e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2010 entries, 0 to 2009\n",
      "Data columns (total 23 columns):\n",
      " #   Column                                           Non-Null Count  Dtype  \n",
      "---  ------                                           --------------  -----  \n",
      " 0   Unnamed: 0                                       2010 non-null   int64  \n",
      " 1   Unnamed: 0.1                                     2010 non-null   int64  \n",
      " 2   Unnamed: 0.1.1                                   2010 non-null   int64  \n",
      " 3   Unnamed: 0.1.1.1                                 2010 non-null   int64  \n",
      " 4   Text                                             2010 non-null   object \n",
      " 5   oh_label                                         2010 non-null   int64  \n",
      " 6   Label                                            2010 non-null   int64  \n",
      " 7   user_links_dropped                               2010 non-null   object \n",
      " 8   spelling_corrected                               2010 non-null   object \n",
      " 9   hashtag_topics                                   2010 non-null   object \n",
      " 10  expanded                                         2010 non-null   object \n",
      " 11  wiki_topics                                      2010 non-null   object \n",
      " 12  clean_regex                                      2010 non-null   object \n",
      " 13  clean_lower                                      2010 non-null   object \n",
      " 14  tokenized_text                                   2010 non-null   object \n",
      " 15  remaining_words                                  2010 non-null   object \n",
      " 16  sentences                                        2010 non-null   object \n",
      " 17  Sentiment Score (Original Text)                  2010 non-null   float64\n",
      " 18  summarised_sentences                             2010 non-null   object \n",
      " 19  Weighted Discourse Fragments (PDTB)              2010 non-null   object \n",
      " 20  Sentiment Score (PDTB split)                     2010 non-null   float64\n",
      " 21  Weighted Discourse Fragments (Dependency split)  2010 non-null   object \n",
      " 22  Sentiment Score (Dependency split)               2010 non-null   float64\n",
      "dtypes: float64(3), int64(6), object(14)\n",
      "memory usage: 361.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# final_sum = []\n",
    "# for each in raw['summarised_sentences']:\n",
    "#     final_sum.append(each)\n",
    "\n",
    "# final_sum\n",
    "raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANzeFIZ7yMug"
   },
   "outputs": [],
   "source": [
    "use_df = raw[['clean_lower','Sentiment Score (Original Text)','Sentiment Score (PDTB split)','Sentiment Score (Dependency split)', 'Label']].copy()\n",
    "use_df\n",
    "use_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwC9XT_12Dfv"
   },
   "outputs": [],
   "source": [
    "df_build = use_df.copy()\n",
    "df_build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qU-tp8327k7z"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTQl3R6v6Jfe"
   },
   "outputs": [],
   "source": [
    "y = df_build['Label']\n",
    "X = df_build[['clean_lower','Sentiment Score (Original Text)','Sentiment Score (PDTB split)','Sentiment Score (Dependency split)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qY3cjVSs8CC4"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1sH5prF3dR7"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEdM52G40U_p"
   },
   "outputs": [],
   "source": [
    "# initialise model and vectorizers\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "NB = MultinomialNB()\n",
    "RF = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=450)\n",
    "XGB = XGBClassifier()\n",
    "LGBM = lgb.LGBMClassifier()\n",
    "LR = LogisticRegression(random_state=450)\n",
    "\n",
    "\n",
    "SVM.fit(X_train,y_train)\n",
    "\n",
    "RF.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "XGB.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "LGBM.fit(X_train,y_train)\n",
    "\n",
    "LR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78VOKCI6wMsW"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled_sem_ori = df_build[['Sentiment Score (Original Text)']]\n",
    "df_scaled_sem_dep = df_build[['Sentiment Score (Dependency split)']]\n",
    "df_scaled_sem_pdtb = df_build[['Sentiment Score (PDTB split)']]\n",
    "print(scaler.fit(df_scaled_sem_ori))\n",
    "print(scaler.fit(df_scaled_sem_dep))\n",
    "print(scaler.fit(df_scaled_sem_pdtb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCvvcj8Q07hJ"
   },
   "outputs": [],
   "source": [
    "scaled_sem_ori = scaler.transform(df_scaled_sem_ori)\n",
    "scaled_sem_dep = scaler.transform(df_scaled_sem_dep)\n",
    "scaled_sem_pdtb = scaler.transform(df_scaled_sem_pdtb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkmSF6Ze2jUT"
   },
   "outputs": [],
   "source": [
    "NB_df = df_build[['clean_lower','Sentiment Score (Original Text)','Sentiment Score (Dependency split)','Sentiment Score (PDTB split)','Label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8FrnfdC26r-"
   },
   "outputs": [],
   "source": [
    "NB_df['scaled_ori'] = scaled_sem_ori\n",
    "NB_df['scaled_dep'] = scaled_sem_dep\n",
    "NB_df['scaled_pdtb'] = scaled_sem_pdtb\n",
    "NB_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBogTlDF3J4g"
   },
   "outputs": [],
   "source": [
    "y_NB = NB_df['Label']\n",
    "X_NB = NB_df[['clean_lower','scaled_ori','scaled_pdtb','scaled_dep']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcsPJw2l3mZp"
   },
   "outputs": [],
   "source": [
    "X_NB_train, X_NB_test, y_NB_train, y_NB_test = train_test_split(X_NB, y_NB, test_size=0.7, random_state=460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_rnLiAZOtXi",
    "outputId": "d2557bae-bae2-4695-f563-34a6a5ce532d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB.fit(X_NB_train,y_NB_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIwG2VOP4Apz"
   },
   "outputs": [],
   "source": [
    "predictions_NB = NB.predict(X_NB_test)\n",
    "nb_accuracy = accuracy_score(predictions_NB, y_test)*100\n",
    "\n",
    "NB_matrix = classification_report(y_NB_test,predictions_NB,labels=[0,1,2])\n",
    "print(\"Accuracy:\", nb_accuracy)\n",
    "print('Classification report : \\n',NB_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzpkY_xO7XY0"
   },
   "outputs": [],
   "source": [
    "predictions_SVM = SVM.predict(X_test)\n",
    "svm_accuracy = accuracy_score(predictions_SVM, y_test)*100\n",
    "\n",
    "SVM_matrix = classification_report(y_test,predictions_SVM)\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print('Classification report : \\n',SVM_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNveEKbl87ns"
   },
   "outputs": [],
   "source": [
    "predictions_RF = RF.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(predictions_RF, y_test)*100\n",
    "RF_matrix = classification_report(y_test,predictions_RF)\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "print('Classification report : \\n',RF_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQcnER-A9Ziq"
   },
   "outputs": [],
   "source": [
    "predictions_XGB = XGB.predict(X_test)\n",
    "\n",
    "xgb_accuracy = accuracy_score(predictions_XGB, y_test)*100\n",
    "XGB_matrix = classification_report(y_test,predictions_XGB)\n",
    "print(\"Accuracy:\", xgb_accuracy)\n",
    "print('Classification report : \\n',XGB_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i9XH10uutTy"
   },
   "outputs": [],
   "source": [
    "predictions_LGBM = LGBM.predict(X_test)\n",
    "\n",
    "lgbm_accuracy = accuracy_score(predictions_LGBM, y_test)*100\n",
    "LGBM_matrix = classification_report(y_test,predictions_LGBM)\n",
    "print(\"Accuracy:\", lgbm_accuracy)\n",
    "print('Classification report : \\n',LGBM_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcb8Ld-Xuv1A"
   },
   "outputs": [],
   "source": [
    "predictions_LR = LR.predict(X_test)\n",
    "\n",
    "lr_accuracy = accuracy_score(predictions_LR, y_test)*100\n",
    "LR_matrix = classification_report(y_test,predictions_LR)\n",
    "print(\"Accuracy:\", lr_accuracy)\n",
    "print('Classification report : \\n',LR_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaVtGZSS6yOo",
    "outputId": "1c2bf646-b33a-4f4f-bb4b-5beda65f5da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vecstack\n",
      "  Downloading vecstack-0.4.0.tar.gz (18 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from vecstack) (1.21.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from vecstack) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from vecstack) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->vecstack) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->vecstack) (1.1.0)\n",
      "Building wheels for collected packages: vecstack\n",
      "  Building wheel for vecstack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for vecstack: filename=vecstack-0.4.0-py3-none-any.whl size=19877 sha256=48fc2f03b35ad8ea860f569d424d29693a554ca9775410c718a353bcc483edc4\n",
      "  Stored in directory: /root/.cache/pip/wheels/28/fe/0c/fe8e43660e3316d7ce204e59a79a72246c0ae9b6c5c79841c8\n",
      "Successfully built vecstack\n",
      "Installing collected packages: vecstack\n",
      "Successfully installed vecstack-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install vecstack\n",
    "from vecstack import stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4fmoccX0LIm"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "final_model = VotingClassifier(estimators=[('svm', SVM), ('xgb', XGB), ('rf', RF)], voting='hard')\n",
    "voting_pipe = Pipeline([\n",
    "                  ('tfidf', column_transformer),\n",
    "                  ('classify', final_model)\n",
    "                ])\n",
    "\n",
    "voting_pipe.fit(X_train,y_train)\n",
    "\n",
    "predictions_voting = voting_pipe.predict(X_test)\n",
    "voting_accuracy = accuracy_score(predictions_voting, y_test)*100\n",
    "voting_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXlBDL6Y_AwX"
   },
   "outputs": [],
   "source": [
    "voting_matrix = classification_report(y_test,predictions_voting,labels=[0,1,2])\n",
    "print(\"Accuracy:\", voting_accuracy)\n",
    "print('Classification report : \\n',voting_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsNkb9j66VYe",
    "outputId": "7b8dc1c5-c3f6-45e6-909f-157ecf3f16f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.407249466950965"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing bagging module\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# initializing the bagging model using XGboost as base model with default parameters\n",
    "svm_bagging = BaggingClassifier(base_estimator=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto'))\n",
    "\n",
    "\n",
    "svm_bagging_pipe = Pipeline([\n",
    "                  ('tfidf', column_transformer),\n",
    "                  ('classify', svm_bagging)])\n",
    "# training model\n",
    "svm_bagging_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predicting the output on the test dataset\n",
    "predictions_bagging_svm = svm_bagging_pipe.predict(X_test)\n",
    "\n",
    "svm_bagging_accuracy = accuracy_score(predictions_bagging_svm, y_test)*100\n",
    "svm_bagging_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4gn7rGGuTen"
   },
   "outputs": [],
   "source": [
    "svm_bagging_matrix = classification_report(y_test,predictions_bagging_svm,labels=[0,1,2])\n",
    "print(\"Accuracy:\", svm_bagging_accuracy)\n",
    "print('Classification report : \\n',svm_bagging_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBkwIjMh7pgA"
   },
   "outputs": [],
   "source": [
    "# importing bagging module\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# initializing the bagging model using XGboost as base model with default parameters\n",
    "xgb_bagging = BaggingClassifier(base_estimator=XGBClassifier())\n",
    "\n",
    "\n",
    "xgb_bagging_pipe = Pipeline([\n",
    "                  ('tfidf', column_transformer),\n",
    "                  ('classify', xgb_bagging)])\n",
    "# training model\n",
    "xgb_bagging_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predicting the output on the test dataset\n",
    "predictions_bagging_xgb = xgb_bagging_pipe.predict(X_test)\n",
    "\n",
    "xgb_bagging_accuracy = accuracy_score(predictions_bagging_xgb, y_test)*100\n",
    "xgb_bagging_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo3HBp1oL9WI"
   },
   "outputs": [],
   "source": [
    "xgb_bagging_matrix = classification_report(y_test,predictions_bagging_xgb,labels=[0,1,2])\n",
    "print(\"Accuracy:\", xgb_bagging_accuracy)\n",
    "print('Classification report : \\n',xgb_bagging_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVsbcHFPsuVe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "clean_lower +  sentiment_score (ori) sentiment_score (dependency split)  + sentiment_score (pdtb)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
