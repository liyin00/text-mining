{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9UEpgUdZU5W",
    "outputId": "f45d15d5-a56d-4b78-bc20-99cfd4a4b181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKkgo5MFVZt1",
    "outputId": "dc824840-8642-4a7f-b815-224a27fdda74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLaP7kthAD3l"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0AbWCRGYqzo"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/Text Mining/Dataset/summarised_weighted_discourse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "htCufrzlY9zJ",
    "outputId": "cad4a2ae-9490-4fbe-d943-bacb7f972689"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5948da82-cf1c-4859-98bd-6526c498dc28\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Text</th>\n",
       "      <th>oh_label</th>\n",
       "      <th>Label</th>\n",
       "      <th>user_links_dropped</th>\n",
       "      <th>spelling_corrected</th>\n",
       "      <th>hashtag_topics</th>\n",
       "      <th>...</th>\n",
       "      <th>clean_lower</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>remaining_words</th>\n",
       "      <th>sentences</th>\n",
       "      <th>Sentiment Score (Original Text)</th>\n",
       "      <th>summarised_sentences</th>\n",
       "      <th>Weighted Discourse Fragments (PDTB)</th>\n",
       "      <th>Sentiment Score (PDTB split)</th>\n",
       "      <th>Weighted Discourse Fragments (Dependency split)</th>\n",
       "      <th>Sentiment Score (Dependency split)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>149970</td>\n",
       "      <td>149988</td>\n",
       "      <td>149988</td>\n",
       "      <td>`  == Clandestine industries ==  Hi - I note y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>` == Clandestine industries == Hi - I note you...</td>\n",
       "      <td>` == Clandestine industries == Hi - I note you...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>clandestine industries  hi  i note you have ...</td>\n",
       "      <td>['clandestine', 'industries', 'hi', 'i', 'note...</td>\n",
       "      <td>['clandestine', 'industries', 'hi', 'note', 'r...</td>\n",
       "      <td>['clandestine industries  hi  i note you have ...</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>['i note you have removed the speedy deletion ...</td>\n",
       "      <td>{'i note you have removed the speedy deletion ...</td>\n",
       "      <td>0.135575</td>\n",
       "      <td>{'i note you have removed the speedy deletion ...</td>\n",
       "      <td>0.27115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>136680</td>\n",
       "      <td>136697</td>\n",
       "      <td>136697</td>\n",
       "      <td>`  ==Sailor Moon Musicals== I tried to add the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>` ==Sailor Moon Musicals== I tried to add the ...</td>\n",
       "      <td>` ==Sailor Moon Musicals== I tried to add the ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>sailor moon musicals i tried to add the clari...</td>\n",
       "      <td>['sailor', 'moon', 'musicals', 'i', 'tried', '...</td>\n",
       "      <td>['sailor', 'moon', 'musicals', 'tried', 'add',...</td>\n",
       "      <td>['sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>['sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>{'sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>0.122480</td>\n",
       "      <td>{'sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>90772</td>\n",
       "      <td>90780</td>\n",
       "      <td>90780</td>\n",
       "      <td>:::That was my point. I wanted to rewrite the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>:::That was my point. I wanted to rewrite the ...</td>\n",
       "      <td>:::That was my point I wanted to rewrite the e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>that was my point i wanted to rewrite the enti...</td>\n",
       "      <td>['that', 'was', 'my', 'point', 'i', 'wanted', ...</td>\n",
       "      <td>['point', 'wanted', 'rewrite', 'entire', 'plot...</td>\n",
       "      <td>['that was my point', 'i wanted to rewrite the...</td>\n",
       "      <td>0.9674</td>\n",
       "      <td>['that was my point', 'i wanted to rewrite the...</td>\n",
       "      <td>{'that was my point': 1, 'i wanted to rewrite ...</td>\n",
       "      <td>0.168560</td>\n",
       "      <td>{'that was my point': 1, 'i wanted to rewrite ...</td>\n",
       "      <td>0.29360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>177127</td>\n",
       "      <td>177147</td>\n",
       "      <td>177147</td>\n",
       "      <td>`  == Channel 4 Documentary ==  Nice work Prio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>` == Channel 4 Documentary == Nice work Priory...</td>\n",
       "      <td>` == Channel 4 Documentary == Nice work Priory...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>channel 4 documentary  nice work prioryman i...</td>\n",
       "      <td>['channel', '4', 'documentary', 'nice', 'work'...</td>\n",
       "      <td>['channel', '4', 'documentary', 'nice', 'work'...</td>\n",
       "      <td>['channel 4 documentary  nice work prioryman',...</td>\n",
       "      <td>0.7641</td>\n",
       "      <td>['channel 4 documentary   nice work prioryman'...</td>\n",
       "      <td>{'channel 4 documentary   nice work prioryman'...</td>\n",
       "      <td>0.271350</td>\n",
       "      <td>{'channel 4 documentary   nice work prioryman'...</td>\n",
       "      <td>0.32250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>122009</td>\n",
       "      <td>122025</td>\n",
       "      <td>122025</td>\n",
       "      <td>Is this species named after Sir David Attenb...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Is this species named after Sir David Attenbor...</td>\n",
       "      <td>Is this species named after Sir David Attenbor...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>is this species named after sir david attenbor...</td>\n",
       "      <td>['is', 'this', 'species', 'named', 'after', 's...</td>\n",
       "      <td>['species', 'named', 'sir', 'david', 'attenbor...</td>\n",
       "      <td>['is this species named after sir david attenb...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>['is this species named after sir david attenb...</td>\n",
       "      <td>{'is this species named after sir david attenb...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'is this species named after sir david attenb...</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>2005</td>\n",
       "      <td>6218</td>\n",
       "      <td>6218</td>\n",
       "      <td>6218</td>\n",
       "      <td>==series scrapped on 4th july??!?==  on http...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>==series scrapped on 4th july??!?== on a few p...</td>\n",
       "      <td>==series scrapped on ith july??!?== on a few p...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>series scrapped on 4th july on a few ppl hav l...</td>\n",
       "      <td>['series', 'scrapped', 'on', '4th', 'july', 'o...</td>\n",
       "      <td>['series', 'scrapped', '4th', 'july', 'ppl', '...</td>\n",
       "      <td>['series scrapped on 4th july', 'on a few ppl ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>['series scrapped on 4th july', 'on a few ppl ...</td>\n",
       "      <td>{'series scrapped on 4th july': 1, 'on a few p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'series scrapped on 4th july': 1, 'on a few p...</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>2006</td>\n",
       "      <td>24195</td>\n",
       "      <td>24196</td>\n",
       "      <td>24196</td>\n",
       "      <td>@Iloveoldtools @Angry_Feminazi No.  Actually, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>No. Actually, she has an economics degree.</td>\n",
       "      <td>not actually she has an economics degrees</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>no actually she has an economics degree</td>\n",
       "      <td>['no', 'actually', 'she', 'has', 'an', 'econom...</td>\n",
       "      <td>['actually', 'economics', 'degree']</td>\n",
       "      <td>['no', 'actually she has an economics degree']</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>['actually she has an economics degree']</td>\n",
       "      <td>{'actually she has an economics degree': 1}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'actually she has an economics degree': 1}</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>2007</td>\n",
       "      <td>10847</td>\n",
       "      <td>10847</td>\n",
       "      <td>10847</td>\n",
       "      <td>Liar, Liar, pants on fire Seriously I looked a...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Liar, Liar, pants on fire Seriously I looked a...</td>\n",
       "      <td>liar liar pants on fire Seriously I looked at ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>liar liar pants on fire seriously i looked at ...</td>\n",
       "      <td>['liar', 'liar', 'pants', 'on', 'fire', 'serio...</td>\n",
       "      <td>['liar', 'liar', 'pants', 'fire', 'seriously',...</td>\n",
       "      <td>['liar liar pants on fire seriously i looked a...</td>\n",
       "      <td>-0.8527</td>\n",
       "      <td>['seriously i looked at your contributions the...</td>\n",
       "      <td>{'seriously i looked at your contributions the...</td>\n",
       "      <td>-0.061100</td>\n",
       "      <td>{'seriously i looked at your contributions the...</td>\n",
       "      <td>-0.44040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>2008</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>I Love to eat rectal yoghurt</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>I Love to eat rectal yoghurt</td>\n",
       "      <td>I Love to eat rectal yoghurt</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>i love to eat rectal yoghurt</td>\n",
       "      <td>['i', 'love', 'to', 'eat', 'rectal', 'yoghurt']</td>\n",
       "      <td>['love', 'eat', 'rectal', 'yoghurt']</td>\n",
       "      <td>['i love to eat rectal yoghurt']</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>['i love to eat rectal yoghurt']</td>\n",
       "      <td>{'i love to eat rectal yoghurt': 1}</td>\n",
       "      <td>0.636900</td>\n",
       "      <td>{'i love to eat rectal yoghurt': 1}</td>\n",
       "      <td>0.63690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>2009</td>\n",
       "      <td>6636</td>\n",
       "      <td>6636</td>\n",
       "      <td>6636</td>\n",
       "      <td>== Dude I'm trying to make a joke for my fri...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>== Dude I'm trying to make a joke for my frien...</td>\n",
       "      <td>== Dude I'm trying to make a joke for my frien...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>dude i am trying to make a joke for my friend...</td>\n",
       "      <td>['dude', 'i', 'am', 'trying', 'to', 'make', 'a...</td>\n",
       "      <td>['dude', 'trying', 'make', 'joke', 'friend', '...</td>\n",
       "      <td>['dude i am trying to make a joke for my frien...</td>\n",
       "      <td>0.4075</td>\n",
       "      <td>['dude i am trying to make a joke for my frien...</td>\n",
       "      <td>{'dude i am trying to make a joke for my frien...</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>{'dude i am trying to make a joke for my frien...</td>\n",
       "      <td>0.26935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2010 rows × 23 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5948da82-cf1c-4859-98bd-6526c498dc28')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5948da82-cf1c-4859-98bd-6526c498dc28 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5948da82-cf1c-4859-98bd-6526c498dc28');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n",
       "0              0        149970          149988            149988   \n",
       "1              1        136680          136697            136697   \n",
       "2              2         90772           90780             90780   \n",
       "3              3        177127          177147            177147   \n",
       "4              4        122009          122025            122025   \n",
       "...          ...           ...             ...               ...   \n",
       "2005        2005          6218            6218              6218   \n",
       "2006        2006         24195           24196             24196   \n",
       "2007        2007         10847           10847             10847   \n",
       "2008        2008           128             128               128   \n",
       "2009        2009          6636            6636              6636   \n",
       "\n",
       "                                                   Text  oh_label  Label  \\\n",
       "0     `  == Clandestine industries ==  Hi - I note y...         0      0   \n",
       "1     `  ==Sailor Moon Musicals== I tried to add the...         0      0   \n",
       "2      :::That was my point. I wanted to rewrite the...         0      0   \n",
       "3     `  == Channel 4 Documentary ==  Nice work Prio...         0      0   \n",
       "4       Is this species named after Sir David Attenb...         0      0   \n",
       "...                                                 ...       ...    ...   \n",
       "2005    ==series scrapped on 4th july??!?==  on http...         1      2   \n",
       "2006  @Iloveoldtools @Angry_Feminazi No.  Actually, ...         1      2   \n",
       "2007  Liar, Liar, pants on fire Seriously I looked a...         1      2   \n",
       "2008                       I Love to eat rectal yoghurt         1      2   \n",
       "2009    == Dude I'm trying to make a joke for my fri...         1      2   \n",
       "\n",
       "                                     user_links_dropped  \\\n",
       "0     ` == Clandestine industries == Hi - I note you...   \n",
       "1     ` ==Sailor Moon Musicals== I tried to add the ...   \n",
       "2     :::That was my point. I wanted to rewrite the ...   \n",
       "3     ` == Channel 4 Documentary == Nice work Priory...   \n",
       "4     Is this species named after Sir David Attenbor...   \n",
       "...                                                 ...   \n",
       "2005  ==series scrapped on 4th july??!?== on a few p...   \n",
       "2006         No. Actually, she has an economics degree.   \n",
       "2007  Liar, Liar, pants on fire Seriously I looked a...   \n",
       "2008                       I Love to eat rectal yoghurt   \n",
       "2009  == Dude I'm trying to make a joke for my frien...   \n",
       "\n",
       "                                     spelling_corrected hashtag_topics  ...  \\\n",
       "0     ` == Clandestine industries == Hi - I note you...             []  ...   \n",
       "1     ` ==Sailor Moon Musicals== I tried to add the ...             []  ...   \n",
       "2     :::That was my point I wanted to rewrite the e...             []  ...   \n",
       "3     ` == Channel 4 Documentary == Nice work Priory...             []  ...   \n",
       "4     Is this species named after Sir David Attenbor...             []  ...   \n",
       "...                                                 ...            ...  ...   \n",
       "2005  ==series scrapped on ith july??!?== on a few p...             []  ...   \n",
       "2006          not actually she has an economics degrees             []  ...   \n",
       "2007  liar liar pants on fire Seriously I looked at ...             []  ...   \n",
       "2008                       I Love to eat rectal yoghurt             []  ...   \n",
       "2009  == Dude I'm trying to make a joke for my frien...             []  ...   \n",
       "\n",
       "                                            clean_lower  \\\n",
       "0       clandestine industries  hi  i note you have ...   \n",
       "1      sailor moon musicals i tried to add the clari...   \n",
       "2     that was my point i wanted to rewrite the enti...   \n",
       "3       channel 4 documentary  nice work prioryman i...   \n",
       "4     is this species named after sir david attenbor...   \n",
       "...                                                 ...   \n",
       "2005  series scrapped on 4th july on a few ppl hav l...   \n",
       "2006            no actually she has an economics degree   \n",
       "2007  liar liar pants on fire seriously i looked at ...   \n",
       "2008                       i love to eat rectal yoghurt   \n",
       "2009   dude i am trying to make a joke for my friend...   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "0     ['clandestine', 'industries', 'hi', 'i', 'note...   \n",
       "1     ['sailor', 'moon', 'musicals', 'i', 'tried', '...   \n",
       "2     ['that', 'was', 'my', 'point', 'i', 'wanted', ...   \n",
       "3     ['channel', '4', 'documentary', 'nice', 'work'...   \n",
       "4     ['is', 'this', 'species', 'named', 'after', 's...   \n",
       "...                                                 ...   \n",
       "2005  ['series', 'scrapped', 'on', '4th', 'july', 'o...   \n",
       "2006  ['no', 'actually', 'she', 'has', 'an', 'econom...   \n",
       "2007  ['liar', 'liar', 'pants', 'on', 'fire', 'serio...   \n",
       "2008    ['i', 'love', 'to', 'eat', 'rectal', 'yoghurt']   \n",
       "2009  ['dude', 'i', 'am', 'trying', 'to', 'make', 'a...   \n",
       "\n",
       "                                        remaining_words  \\\n",
       "0     ['clandestine', 'industries', 'hi', 'note', 'r...   \n",
       "1     ['sailor', 'moon', 'musicals', 'tried', 'add',...   \n",
       "2     ['point', 'wanted', 'rewrite', 'entire', 'plot...   \n",
       "3     ['channel', '4', 'documentary', 'nice', 'work'...   \n",
       "4     ['species', 'named', 'sir', 'david', 'attenbor...   \n",
       "...                                                 ...   \n",
       "2005  ['series', 'scrapped', '4th', 'july', 'ppl', '...   \n",
       "2006                ['actually', 'economics', 'degree']   \n",
       "2007  ['liar', 'liar', 'pants', 'fire', 'seriously',...   \n",
       "2008               ['love', 'eat', 'rectal', 'yoghurt']   \n",
       "2009  ['dude', 'trying', 'make', 'joke', 'friend', '...   \n",
       "\n",
       "                                              sentences  \\\n",
       "0     ['clandestine industries  hi  i note you have ...   \n",
       "1     ['sailor moon musicals i tried to add the clar...   \n",
       "2     ['that was my point', 'i wanted to rewrite the...   \n",
       "3     ['channel 4 documentary  nice work prioryman',...   \n",
       "4     ['is this species named after sir david attenb...   \n",
       "...                                                 ...   \n",
       "2005  ['series scrapped on 4th july', 'on a few ppl ...   \n",
       "2006     ['no', 'actually she has an economics degree']   \n",
       "2007  ['liar liar pants on fire seriously i looked a...   \n",
       "2008                   ['i love to eat rectal yoghurt']   \n",
       "2009  ['dude i am trying to make a joke for my frien...   \n",
       "\n",
       "     Sentiment Score (Original Text)  \\\n",
       "0                             0.3182   \n",
       "1                             0.3612   \n",
       "2                             0.9674   \n",
       "3                             0.7641   \n",
       "4                             0.0000   \n",
       "...                              ...   \n",
       "2005                          0.0000   \n",
       "2006                          0.0000   \n",
       "2007                         -0.8527   \n",
       "2008                          0.6369   \n",
       "2009                          0.4075   \n",
       "\n",
       "                                   summarised_sentences  \\\n",
       "0     ['i note you have removed the speedy deletion ...   \n",
       "1     ['sailor moon musicals i tried to add the clar...   \n",
       "2     ['that was my point', 'i wanted to rewrite the...   \n",
       "3     ['channel 4 documentary   nice work prioryman'...   \n",
       "4     ['is this species named after sir david attenb...   \n",
       "...                                                 ...   \n",
       "2005  ['series scrapped on 4th july', 'on a few ppl ...   \n",
       "2006           ['actually she has an economics degree']   \n",
       "2007  ['seriously i looked at your contributions the...   \n",
       "2008                   ['i love to eat rectal yoghurt']   \n",
       "2009  ['dude i am trying to make a joke for my frien...   \n",
       "\n",
       "                    Weighted Discourse Fragments (PDTB)  \\\n",
       "0     {'i note you have removed the speedy deletion ...   \n",
       "1     {'sailor moon musicals i tried to add the clar...   \n",
       "2     {'that was my point': 1, 'i wanted to rewrite ...   \n",
       "3     {'channel 4 documentary   nice work prioryman'...   \n",
       "4     {'is this species named after sir david attenb...   \n",
       "...                                                 ...   \n",
       "2005  {'series scrapped on 4th july': 1, 'on a few p...   \n",
       "2006        {'actually she has an economics degree': 1}   \n",
       "2007  {'seriously i looked at your contributions the...   \n",
       "2008                {'i love to eat rectal yoghurt': 1}   \n",
       "2009  {'dude i am trying to make a joke for my frien...   \n",
       "\n",
       "      Sentiment Score (PDTB split)  \\\n",
       "0                         0.135575   \n",
       "1                         0.122480   \n",
       "2                         0.168560   \n",
       "3                         0.271350   \n",
       "4                         0.000000   \n",
       "...                            ...   \n",
       "2005                      0.000000   \n",
       "2006                      0.000000   \n",
       "2007                     -0.061100   \n",
       "2008                      0.636900   \n",
       "2009                      0.080900   \n",
       "\n",
       "        Weighted Discourse Fragments (Dependency split)  \\\n",
       "0     {'i note you have removed the speedy deletion ...   \n",
       "1     {'sailor moon musicals i tried to add the clar...   \n",
       "2     {'that was my point': 1, 'i wanted to rewrite ...   \n",
       "3     {'channel 4 documentary   nice work prioryman'...   \n",
       "4     {'is this species named after sir david attenb...   \n",
       "...                                                 ...   \n",
       "2005  {'series scrapped on 4th july': 1, 'on a few p...   \n",
       "2006        {'actually she has an economics degree': 1}   \n",
       "2007  {'seriously i looked at your contributions the...   \n",
       "2008                {'i love to eat rectal yoghurt': 1}   \n",
       "2009  {'dude i am trying to make a joke for my frien...   \n",
       "\n",
       "     Sentiment Score (Dependency split)  \n",
       "0                               0.27115  \n",
       "1                               0.00000  \n",
       "2                               0.29360  \n",
       "3                               0.32250  \n",
       "4                               0.00000  \n",
       "...                                 ...  \n",
       "2005                            0.00000  \n",
       "2006                            0.00000  \n",
       "2007                           -0.44040  \n",
       "2008                            0.63690  \n",
       "2009                            0.26935  \n",
       "\n",
       "[2010 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dt1Q_xTEBA29",
    "outputId": "e08f6c13-8aab-4daf-887e-4d030587b6cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2010 entries, 0 to 2009\n",
      "Data columns (total 23 columns):\n",
      " #   Column                                           Non-Null Count  Dtype  \n",
      "---  ------                                           --------------  -----  \n",
      " 0   Unnamed: 0                                       2010 non-null   int64  \n",
      " 1   Unnamed: 0.1                                     2010 non-null   int64  \n",
      " 2   Unnamed: 0.1.1                                   2010 non-null   int64  \n",
      " 3   Unnamed: 0.1.1.1                                 2010 non-null   int64  \n",
      " 4   Text                                             2010 non-null   object \n",
      " 5   oh_label                                         2010 non-null   int64  \n",
      " 6   Label                                            2010 non-null   int64  \n",
      " 7   user_links_dropped                               2010 non-null   object \n",
      " 8   spelling_corrected                               2010 non-null   object \n",
      " 9   hashtag_topics                                   2010 non-null   object \n",
      " 10  expanded                                         2010 non-null   object \n",
      " 11  wiki_topics                                      2010 non-null   object \n",
      " 12  clean_regex                                      2010 non-null   object \n",
      " 13  clean_lower                                      2010 non-null   object \n",
      " 14  tokenized_text                                   2010 non-null   object \n",
      " 15  remaining_words                                  2010 non-null   object \n",
      " 16  sentences                                        2010 non-null   object \n",
      " 17  Sentiment Score (Original Text)                  2010 non-null   float64\n",
      " 18  summarised_sentences                             2010 non-null   object \n",
      " 19  Weighted Discourse Fragments (PDTB)              2010 non-null   object \n",
      " 20  Sentiment Score (PDTB split)                     2010 non-null   float64\n",
      " 21  Weighted Discourse Fragments (Dependency split)  2010 non-null   object \n",
      " 22  Sentiment Score (Dependency split)               2010 non-null   float64\n",
      "dtypes: float64(3), int64(6), object(14)\n",
      "memory usage: 361.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPqVsjqahM1a",
    "outputId": "424a75a9-bd7f-4169-f126-7ef7790a731b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2010 entries, 0 to 2009\n",
      "Data columns (total 3 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   summarised_sentences                2010 non-null   object \n",
      " 1   Sentiment Score (Dependency split)  2010 non-null   float64\n",
      " 2   Label                               2010 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 47.2+ KB\n"
     ]
    }
   ],
   "source": [
    "use_df2 = df[['summarised_sentences','Sentiment Score (Dependency split)','Label']].copy()\n",
    "use_df2\n",
    "use_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4fLxQLo0GXzc",
    "outputId": "c636721b-ff31-40f6-f524-bba5be07c3ba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-752032f2-90aa-47b7-99b9-157a21e9d217\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summarised_sentences</th>\n",
       "      <th>Sentiment Score (Dependency split)</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['i note you have removed the speedy deletion ...</td>\n",
       "      <td>0.27115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['that was my point', 'i wanted to rewrite the...</td>\n",
       "      <td>0.29360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['channel 4 documentary   nice work prioryman'...</td>\n",
       "      <td>0.32250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['is this species named after sir david attenb...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>['series scrapped on 4th july', 'on a few ppl ...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>['actually she has an economics degree']</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>['seriously i looked at your contributions the...</td>\n",
       "      <td>-0.44040</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>['i love to eat rectal yoghurt']</td>\n",
       "      <td>0.63690</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>['dude i am trying to make a joke for my frien...</td>\n",
       "      <td>0.26935</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2010 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-752032f2-90aa-47b7-99b9-157a21e9d217')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-752032f2-90aa-47b7-99b9-157a21e9d217 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-752032f2-90aa-47b7-99b9-157a21e9d217');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                   summarised_sentences  \\\n",
       "0     ['i note you have removed the speedy deletion ...   \n",
       "1     ['sailor moon musicals i tried to add the clar...   \n",
       "2     ['that was my point', 'i wanted to rewrite the...   \n",
       "3     ['channel 4 documentary   nice work prioryman'...   \n",
       "4     ['is this species named after sir david attenb...   \n",
       "...                                                 ...   \n",
       "2005  ['series scrapped on 4th july', 'on a few ppl ...   \n",
       "2006           ['actually she has an economics degree']   \n",
       "2007  ['seriously i looked at your contributions the...   \n",
       "2008                   ['i love to eat rectal yoghurt']   \n",
       "2009  ['dude i am trying to make a joke for my frien...   \n",
       "\n",
       "      Sentiment Score (Dependency split)  Label  \n",
       "0                                0.27115      0  \n",
       "1                                0.00000      0  \n",
       "2                                0.29360      0  \n",
       "3                                0.32250      0  \n",
       "4                                0.00000      0  \n",
       "...                                  ...    ...  \n",
       "2005                             0.00000      2  \n",
       "2006                             0.00000      2  \n",
       "2007                            -0.44040      2  \n",
       "2008                             0.63690      2  \n",
       "2009                             0.26935      2  \n",
       "\n",
       "[2010 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_build2 = use_df2.copy()\n",
    "df_build2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKmhM_nfGd9d"
   },
   "outputs": [],
   "source": [
    "y = df_build2['Label']\n",
    "X = df_build2[['summarised_sentences','Sentiment Score (Dependency split)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtBGtE0NGubg",
    "outputId": "adeeabae-aad5-4785-bbdb-898aba6b3f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "df_scaled_sem_dep = df_build2[['Sentiment Score (Dependency split)']]\n",
    "print(scaler.fit(df_scaled_sem_dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZO-HR9HHQJ-"
   },
   "outputs": [],
   "source": [
    "scaled_sem_dep = scaler.transform(df_scaled_sem_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZjUOjKdG1SA"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRI-u6H9HEOV"
   },
   "outputs": [],
   "source": [
    "NB_df = df_build2[['summarised_sentences','Label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0ijnrjAHK3w"
   },
   "outputs": [],
   "source": [
    "NB_df['scaled_dep'] = scaled_sem_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "J3t87PpyHgYG",
    "outputId": "64d98404-0a95-4325-f621-71ce97e98e76"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-fa85a53f-e9be-4ea5-9220-2ae0f20cb525\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summarised_sentences</th>\n",
       "      <th>Label</th>\n",
       "      <th>scaled_dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['i note you have removed the speedy deletion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.635607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['sailor moon musicals i tried to add the clar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['that was my point', 'i wanted to rewrite the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.646832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['channel 4 documentary   nice work prioryman'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.661283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['is this species named after sir david attenb...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>['series scrapped on 4th july', 'on a few ppl ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>['actually she has an economics degree']</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>['seriously i looked at your contributions the...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.279814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>['i love to eat rectal yoghurt']</td>\n",
       "      <td>2</td>\n",
       "      <td>0.818491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>['dude i am trying to make a joke for my frien...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.634707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2010 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa85a53f-e9be-4ea5-9220-2ae0f20cb525')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-fa85a53f-e9be-4ea5-9220-2ae0f20cb525 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-fa85a53f-e9be-4ea5-9220-2ae0f20cb525');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                   summarised_sentences  Label  scaled_dep\n",
       "0     ['i note you have removed the speedy deletion ...      0    0.635607\n",
       "1     ['sailor moon musicals i tried to add the clar...      0    0.500025\n",
       "2     ['that was my point', 'i wanted to rewrite the...      0    0.646832\n",
       "3     ['channel 4 documentary   nice work prioryman'...      0    0.661283\n",
       "4     ['is this species named after sir david attenb...      0    0.500025\n",
       "...                                                 ...    ...         ...\n",
       "2005  ['series scrapped on 4th july', 'on a few ppl ...      2    0.500025\n",
       "2006           ['actually she has an economics degree']      2    0.500025\n",
       "2007  ['seriously i looked at your contributions the...      2    0.279814\n",
       "2008                   ['i love to eat rectal yoghurt']      2    0.818491\n",
       "2009  ['dude i am trying to make a joke for my frien...      2    0.634707\n",
       "\n",
       "[2010 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuG09j9EHaM2"
   },
   "outputs": [],
   "source": [
    "y_NB = NB_df['Label']\n",
    "X_NB = NB_df[['summarised_sentences','scaled_dep']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOtGAKddHpYn"
   },
   "outputs": [],
   "source": [
    "X_NB_train, X_NB_test, y_NB_train, y_NB_test = train_test_split(X_NB, y_NB, test_size=0.7, random_state=460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXrzP7jvHqoS",
    "outputId": "69e98bfa-e42a-426d-a947-5d25a795f305"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('tfidf1', TfidfVectorizer(),\n",
       "                                                  'summarised_sentences')])),\n",
       "                ('classify', MultinomialNB())])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_pipe = Pipeline([\n",
    "                  ('tfidf', column_transformer),\n",
    "                  ('classify', NB)\n",
    "                ])\n",
    "NB_pipe.fit(X_NB_train,y_NB_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfQjU4ZxHuLt",
    "outputId": "d1c9d689-f0dd-4699-9c9d-8f65f2af377f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('tfidf1', TfidfVectorizer(),\n",
       "                                                  'summarised_sentences')])),\n",
       "                ('classify', LogisticRegression(random_state=450))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise model and vectorizers\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "NB = MultinomialNB()\n",
    "RF = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=450)\n",
    "XGB = XGBClassifier()\n",
    "LGBM = lgb.LGBMClassifier()\n",
    "LR = LogisticRegression(random_state=450)\n",
    "vectorizer1 = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# construct the column transfomer\n",
    "column_transformer = ColumnTransformer(\n",
    "    [('tfidf1', vectorizer1, 'summarised_sentences')],\n",
    "    remainder='passthrough')\n",
    "\n",
    "# fit the model\n",
    "SVM_pipe = Pipeline([\n",
    "                  ('tfidf', column_transformer),\n",
    "                  ('classify', SVM)\n",
    "                ])\n",
    "SVM_pipe.fit(X_train,y_train)\n",
    "\n",
    "# NB_pipe = Pipeline([\n",
    "#                   ('tfidf', column_transformer),\n",
    "#                   ('classify', NB)\n",
    "#                 ])\n",
    "# NB_pipe.fit(X_train,y_train)\n",
    "\n",
    "RF_pipe = Pipeline([\n",
    "                  ('tfidf', column_transformer),\n",
    "                  ('classify', RF)\n",
    "                ])\n",
    "RF_pipe.fit(X_train,y_train)\n",
    "\n",
    "XGB_pipe = Pipeline([\n",
    "                  ('tfidf', column_transformer),\n",
    "                  ('classify', XGB)\n",
    "                ])\n",
    "XGB_pipe.fit(X_train,y_train)\n",
    "\n",
    "LGBM_pipe = Pipeline([\n",
    "                  ('tfidf', column_transformer),\n",
    "                  ('classify', LGBM)\n",
    "                ])\n",
    "LGBM_pipe.fit(X_train,y_train)\n",
    "\n",
    "LR_pipe = Pipeline([\n",
    "                  ('tfidf', column_transformer),\n",
    "                  ('classify', LR)\n",
    "                ])\n",
    "LR_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_Nj53fcH5tI",
    "outputId": "1786e1ae-d129-4b28-d017-992268ac604c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.26510305614783\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69       475\n",
      "           1       0.58      0.52      0.55       463\n",
      "           2       0.62      0.55      0.58       469\n",
      "\n",
      "    accuracy                           0.61      1407\n",
      "   macro avg       0.61      0.61      0.61      1407\n",
      "weighted avg       0.61      0.61      0.61      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_NB = NB_pipe.predict(X_NB_test)\n",
    "nb_accuracy = accuracy_score(predictions_NB, y_test)*100\n",
    "\n",
    "NB_matrix = classification_report(y_NB_test,predictions_NB,labels=[0,1,2])\n",
    "print(\"Accuracy:\", nb_accuracy)\n",
    "print('Classification report : \\n',NB_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjcbMGfrIA6_",
    "outputId": "21c0620c-6048-4562-a859-2a3739ac6a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.00071073205402\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66       475\n",
      "           1       0.75      0.19      0.31       463\n",
      "           2       0.52      0.76      0.62       469\n",
      "\n",
      "    accuracy                           0.57      1407\n",
      "   macro avg       0.62      0.57      0.53      1407\n",
      "weighted avg       0.62      0.57      0.53      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_SVM = SVM_pipe.predict(X_NB_test)\n",
    "svm_accuracy = accuracy_score(predictions_SVM, y_NB_test)*100\n",
    "\n",
    "SVM_matrix = classification_report(y_NB_test,predictions_SVM,labels=[0,1,2])\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print('Classification report : \\n',SVM_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTlLqxQoIFaW",
    "outputId": "b1c2a2ae-2e80-4365-da12-eb2fa77e2542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.447050461975834\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.64      0.60       475\n",
      "           1       0.56      0.30      0.39       463\n",
      "           2       0.50      0.65      0.57       469\n",
      "\n",
      "    accuracy                           0.53      1407\n",
      "   macro avg       0.54      0.53      0.52      1407\n",
      "weighted avg       0.54      0.53      0.52      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_LGBM = LGBM_pipe.predict(X_NB_test)\n",
    "\n",
    "lgbm_accuracy = accuracy_score(predictions_LGBM, y_NB_test)*100\n",
    "LGBM_matrix = classification_report(y_NB_test,predictions_LGBM)\n",
    "print(\"Accuracy:\", lgbm_accuracy)\n",
    "print('Classification report : \\n',LGBM_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdufQIYyIGB1",
    "outputId": "67af5f9d-f4f2-461c-e6e3-fcf487f025c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.57924662402275\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.65      0.62       475\n",
      "           1       0.64      0.27      0.38       463\n",
      "           2       0.50      0.75      0.60       469\n",
      "\n",
      "    accuracy                           0.56      1407\n",
      "   macro avg       0.58      0.55      0.53      1407\n",
      "weighted avg       0.58      0.56      0.53      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_XGB = XGB_pipe.predict(X_NB_test)\n",
    "\n",
    "xgb_accuracy = accuracy_score(predictions_XGB, y_NB_test)*100\n",
    "XGB_matrix = classification_report(y_NB_test,predictions_XGB,labels=[0,1,2])\n",
    "print(\"Accuracy:\", xgb_accuracy)\n",
    "print('Classification report : \\n',XGB_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iz4eE9iYIezh",
    "outputId": "10c2b3b1-e2d4-4700-dcea-cb41446d64a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.508173418621176\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.78      0.65       475\n",
      "           1       0.87      0.12      0.21       463\n",
      "           2       0.52      0.75      0.62       469\n",
      "\n",
      "    accuracy                           0.56      1407\n",
      "   macro avg       0.65      0.55      0.49      1407\n",
      "weighted avg       0.65      0.56      0.49      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_LR = LR_pipe.predict(X_NB_test)\n",
    "\n",
    "lr_accuracy = accuracy_score(predictions_LR, y_NB_test)*100\n",
    "LR_matrix = classification_report(y_NB_test,predictions_LR)\n",
    "print(\"Accuracy:\", lr_accuracy)\n",
    "print('Classification report : \\n',LR_matrix)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Glenda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
